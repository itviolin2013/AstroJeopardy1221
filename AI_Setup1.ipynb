{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f4880536-ff3f-423d-8d7e-d91a3be202c9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded Astronomy 1221 key\n",
            "Successfully found .gitignore in the current directory\n",
            "Confirmed that .gitignore has the .env exclusion\n"
          ]
        }
      ],
      "source": [
        "# things that i am writing are here and popping up hopefully\n",
        "# We will use this to suppress some warnings that are not important\n",
        "import warnings\n",
        "\n",
        "# Suppress specific Pydantic warnings that clutter the output\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydantic\")\n",
        "\n",
        "# We will use dotenv to read the .env file\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# This import will return an error if LiteLLM is not installed \n",
        "import litellm\n",
        "import os\n",
        "\n",
        "# Use this to measure response time\n",
        "import time\n",
        "\n",
        "# URL of Ohio State's LiteLLM proxy server\n",
        "custom_api_base = \"https://litellmproxy.osu-ai.org\" \n",
        "\n",
        "# Our API key for Astronomy 1221 (keep this private to our class)\n",
        "astro1221_key = os.getenv(\"ASTRO1221_API_KEY\")\n",
        "if astro1221_key:\n",
        "    print(\"Successfully loaded Astronomy 1221 key\")\n",
        "else:\n",
        "    print(\"Error: did not find key. Check that .env exists in the same folder/directory as your class notebooks\")\n",
        "\n",
        "# Check that .gitignore exists in this directory\n",
        "if os.path.isfile('.gitignore'):\n",
        "    print(\"Successfully found .gitignore in the current directory\")\n",
        "else:\n",
        "    print(\"Error: Did not find .gitignore. Please download .gitignore from carmen and put in the same folder/directory as your class notebooks.\")\n",
        "\n",
        "with open('.gitignore', 'r') as f:\n",
        "    content = f.read()\n",
        "    if '.env' in content:\n",
        "        print(\"Confirmed that .gitignore has the .env exclusion\")\n",
        "    else: \n",
        "        print(\"Error: Did not find .env in .gitignore. Please download .gitignore from carmen and put with your class notebooks.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "38a2eb64",
      "metadata": {},
      "outputs": [],
      "source": [
        "def prompt_llm(messages, model=\"openai/GPT-4.1-mini\", temperature=0.2, max_tokens=1000):\n",
        "    \"\"\"\n",
        "    Send a prompt or conversation to an LLM using LiteLLM and return the response.\n",
        "\n",
        "    Parameters:\n",
        "        messages: Either a string (single user prompt) or a list of message dicts with\n",
        "                  \"role\" and \"content\". If a string, formatted as [{\"role\": \"user\", \"content\": messages}].\n",
        "        model (str, optional): The name of the model to use. Defaults to \"openai/GPT-4.1-mini\".\n",
        "        temperature (float, optional): Value between 0 and 2; higher values make output more random. Defaults to 0.2.\n",
        "        max_tokens (int, optional): Maximum number of tokens to generate in the completion. Must be a positive integer. Defaults to 1000.\n",
        "\n",
        "    Prints the answer returned by the model.\n",
        "    \n",
        "    Returns:\n",
        "        response: The full response object from LiteLLM.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If `temperature` is not in [0, 2] or `max_tokens` is not a positive integer.\n",
        "    \"\"\"\n",
        "    # If messages is a string, format it as a single user message\n",
        "    if isinstance(messages, str):\n",
        "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
        "    # Validate temperature\n",
        "    if not (isinstance(temperature, (int, float)) and 0 <= temperature <= 2):\n",
        "        raise ValueError(\"temperature must be a float between 0 and 2 (inclusive).\")\n",
        "    # Validate max_tokens\n",
        "    if not (isinstance(max_tokens, int) and max_tokens > 0):\n",
        "        raise ValueError(\"max_tokens must be a positive integer.\")\n",
        "\n",
        "    try: \n",
        "        print(\"Contacting LLM via University Server...\")\n",
        "\n",
        "        response = litellm.completion(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            api_base=custom_api_base,\n",
        "            api_key=astro1221_key,\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens\n",
        "        )\n",
        "\n",
        "        answer = response['choices'][0]['message']['content']\n",
        "        print(f\"\\nSUCCESS! Here is the answer from {model}:\\n\")\n",
        "        print(answer)\n",
        "        print(\"\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR: Could not connect. Details:\\n{e}\")    \n",
        "        response = None\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "34660474",
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_response_metadata(response):\n",
        "    '''\n",
        "    Convert the response to a dictionary\n",
        "    Print information about token usage and costs\n",
        "    '''\n",
        "    \n",
        "    # Here are the top level keys\n",
        "    response_dict = response.model_dump()\n",
        "    # print(f\"Top-level keys: {response_dict.keys()}\\n\")\n",
        "    \n",
        "    # Here are more details: \n",
        "    # 1. Get the exact model version used by the server\n",
        "    used_model = response.model\n",
        "    \n",
        "    # 2. Extract token counts from the 'usage' attribute\n",
        "    input_tokens = response.usage.prompt_tokens\n",
        "    output_tokens = response.usage.completion_tokens\n",
        "    total_tokens = response.usage.total_tokens\n",
        "    \n",
        "    # 3. Calculate the cost (LiteLLM does the math based on current rates)\n",
        "    cost = litellm.completion_cost(completion_response=response)\n",
        "    \n",
        "    print(f\"--- Query Metadata ---\")\n",
        "    print(f\"Model:        {used_model}\")\n",
        "    print(f\"Input Tokens: {input_tokens}\")\n",
        "    print(f\"Output Tokens:{output_tokens}\")\n",
        "    print(f\"Total Tokens: {total_tokens}\")\n",
        "    print(f\"Estimated Cost: ${cost:.6f}\") # Showing 6 decimal places for small queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb46962a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "astro_jeopardy_answers.csv exists in the current directory.\n",
            "Data array created successfully.\n",
            "The array's shape is (6, 6).\n",
            "['Exoplanets' 'HD189733b' 'Wasp 12-b' 'Tres 2b' 'GJ 1132b' '51 Pegasi b']\n",
            "['Exoplanets' 'Galaxies' 'Constellations' 'Planets' 'Moons'\n",
            " 'Astronomers/Physicists/Astronauts']\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# check path location to make sure directory is correct\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "# make sure the file is in our directory\n",
        "if os.path.exists(\"astro_jeopardy_answers.csv\"):\n",
        "    print(\"astro_jeopardy_answers.csv exists in the current directory.\")\n",
        "else:\n",
        "    print(\"astro_jeopardy_answers.csv does not exist in the current directory.\")\n",
        "\n",
        "# read the file\n",
        "with open(\"astro_jeopardy_answers.csv\", \"r\") as file:\n",
        "    reader = csv.reader(file)\n",
        "    try:\n",
        "        data_array = np.genfromtxt(\"astro_jeopardy_answers.csv\", delimiter=\",\", dtype=str)\n",
        "        print(\"Data array created successfully.\")\n",
        "        print(f\"The array's shape is {data_array.shape}.\")\n",
        "        print(data_array[:, 0])\n",
        "        print(data_array[0, :])\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating data array: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9a38b6c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contacting LLM via University Server...\n",
            "\n",
            "SUCCESS! Here is the answer from openai/GPT-4.1-mini:\n",
            "\n",
            "This third largest constellation, also called the Great Bear, represents Callisto, a nymph loved by Zeus in Greek mythology.\n",
            "\n",
            "\n",
            "--- Query Metadata ---\n",
            "Model:        gpt-4.1-mini-2025-04-14\n",
            "Input Tokens: 113\n",
            "Output Tokens:26\n",
            "Total Tokens: 139\n",
            "Estimated Cost: $0.000087\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "chat_assignment = f\"\"\"You are Alex Trebek and hosting a game of Astronomy-themed Jeopardy. Generate one Jeopardy-style\n",
        "     clue using the given facts, with the answer being {data_array[1,2]} in the following category: {data_array[0,2]}. \n",
        "     Do not mention the answer in the prompt, and only include the clue in your response.\"\"\"\n",
        "prompt = \"Ursa Major is also known as the Great Bear and the third largest constellation. In Greek mythology, this bear is Callisto, a nymph of Artemis that Zeus is madly in love with.\"\n",
        "# Next steps: make a for loop to run through each individual clue and answer. Then append each one to a list and then make a numpy array. \n",
        "\n",
        "messages = [{\"role\": \"system\", \"content\": chat_assignment}, \n",
        "{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "response = prompt_llm(messages)\n",
        "print(show_response_metadata(response))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
