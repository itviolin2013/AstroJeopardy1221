{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f4880536-ff3f-423d-8d7e-d91a3be202c9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: did not find key. Check that .env exists in the same folder/directory as your class notebooks\n",
            "Successfully found .gitignore in the current directory\n",
            "Confirmed that .gitignore has the .env exclusion\n"
          ]
        }
      ],
      "source": [
        "# things that i am writing are here and popping up hopefully\n",
        "# We will use this to suppress some warnings that are not important\n",
        "import warnings\n",
        "\n",
        "# Suppress specific Pydantic warnings that clutter the output\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydantic\")\n",
        "\n",
        "# We will use dotenv to read the .env file\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# This import will return an error if LiteLLM is not installed \n",
        "import litellm\n",
        "import os\n",
        "\n",
        "# Use this to measure response time\n",
        "import time\n",
        "\n",
        "# URL of Ohio State's LiteLLM proxy server\n",
        "custom_api_base = \"https://litellmproxy.osu-ai.org\" \n",
        "\n",
        "# Our API key for Astronomy 1221 (keep this private to our class)\n",
        "astro1221_key = os.getenv(\"ASTRO1221_API_KEY\")\n",
        "if astro1221_key:\n",
        "    print(\"Successfully loaded Astronomy 1221 key\")\n",
        "else:\n",
        "    print(\"Error: did not find key. Check that .env exists in the same folder/directory as your class notebooks\")\n",
        "\n",
        "# Check that .gitignore exists in this directory\n",
        "if os.path.isfile('.gitignore'):\n",
        "    print(\"Successfully found .gitignore in the current directory\")\n",
        "else:\n",
        "    print(\"Error: Did not find .gitignore. Please download .gitignore from carmen and put in the same folder/directory as your class notebooks.\")\n",
        "\n",
        "with open('.gitignore', 'r') as f:\n",
        "    content = f.read()\n",
        "    if '.env' in content:\n",
        "        print(\"Confirmed that .gitignore has the .env exclusion\")\n",
        "    else: \n",
        "        print(\"Error: Did not find .env in .gitignore. Please download .gitignore from carmen and put with your class notebooks.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "38a2eb64",
      "metadata": {},
      "outputs": [],
      "source": [
        "def prompt_llm(messages, model=\"openai/GPT-4.1-mini\", temperature=0.2, max_tokens=1000):\n",
        "    \"\"\"\n",
        "    Send a prompt or conversation to an LLM using LiteLLM and return the response.\n",
        "\n",
        "    Parameters:\n",
        "        messages: Either a string (single user prompt) or a list of message dicts with\n",
        "                  \"role\" and \"content\". If a string, formatted as [{\"role\": \"user\", \"content\": messages}].\n",
        "        model (str, optional): The name of the model to use. Defaults to \"openai/GPT-4.1-mini\".\n",
        "        temperature (float, optional): Value between 0 and 2; higher values make output more random. Defaults to 0.2.\n",
        "        max_tokens (int, optional): Maximum number of tokens to generate in the completion. Must be a positive integer. Defaults to 1000.\n",
        "\n",
        "    Prints the answer returned by the model.\n",
        "    \n",
        "    Returns:\n",
        "        response: The full response object from LiteLLM.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If `temperature` is not in [0, 2] or `max_tokens` is not a positive integer.\n",
        "    \"\"\"\n",
        "    # If messages is a string, format it as a single user message\n",
        "    if isinstance(messages, str):\n",
        "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
        "    # Validate temperature\n",
        "    if not (isinstance(temperature, (int, float)) and 0 <= temperature <= 2):\n",
        "        raise ValueError(\"temperature must be a float between 0 and 2 (inclusive).\")\n",
        "    # Validate max_tokens\n",
        "    if not (isinstance(max_tokens, int) and max_tokens > 0):\n",
        "        raise ValueError(\"max_tokens must be a positive integer.\")\n",
        "\n",
        "    try: \n",
        "        print(\"Contacting LLM via University Server...\")\n",
        "\n",
        "        response = litellm.completion(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            api_base=custom_api_base,\n",
        "            api_key=astro1221_key,\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens\n",
        "        )\n",
        "\n",
        "        answer = response['choices'][0]['message']['content']\n",
        "        print(f\"\\nSUCCESS! Here is the answer from {model}:\\n\")\n",
        "        print(answer)\n",
        "        print(\"\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR: Could not connect. Details:\\n{e}\")    \n",
        "        response = None\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "34660474",
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_response_metadata(response):\n",
        "    '''\n",
        "    Convert the response to a dictionary\n",
        "    Print information about token usage and costs\n",
        "    '''\n",
        "    \n",
        "    # Here are the top level keys\n",
        "    response_dict = response.model_dump()\n",
        "    # print(f\"Top-level keys: {response_dict.keys()}\\n\")\n",
        "    \n",
        "    # Here are more details: \n",
        "    # 1. Get the exact model version used by the server\n",
        "    used_model = response.model\n",
        "    \n",
        "    # 2. Extract token counts from the 'usage' attribute\n",
        "    input_tokens = response.usage.prompt_tokens\n",
        "    output_tokens = response.usage.completion_tokens\n",
        "    total_tokens = response.usage.total_tokens\n",
        "    \n",
        "    # 3. Calculate the cost (LiteLLM does the math based on current rates)\n",
        "    cost = litellm.completion_cost(completion_response=response)\n",
        "    \n",
        "    print(f\"--- Query Metadata ---\")\n",
        "    print(f\"Model:        {used_model}\")\n",
        "    print(f\"Input Tokens: {input_tokens}\")\n",
        "    print(f\"Output Tokens:{output_tokens}\")\n",
        "    print(f\"Total Tokens: {total_tokens}\")\n",
        "    print(f\"Estimated Cost: ${cost:.6f}\") # Showing 6 decimal places for small queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fb46962a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "astro_jeopardy_answers.csv exists in the current directory.\n",
            "Data array created successfully.\n",
            "The array's shape is (6, 6).\n",
            "[['Exoplanets' 'Galaxies' 'Constellations' 'Planets' 'Moons'\n",
            "  'Astronomers/Physicists/Astronauts']\n",
            " ['HD189733b' 'the Milky Way' 'Ursa Major' 'Neptune' 'Titan'\n",
            "  'Robert Henry Lawerence Jr.']\n",
            " ['Wasp 12-b' 'M31' 'Orion' 'Saturn' 'Europa' 'Kathrine Johnson']\n",
            " ['Tres 2b' 'M32' 'Cassiopeia' 'Venus' 'Enceladus' 'Sally Ride']\n",
            " ['GJ 1132b' 'M33' 'Pegasus' 'Uranus' 'Triton' 'Sunita Williams']\n",
            " ['51 Pegasi b' 'M64' 'Sagittarius' 'Earth' 'Phobos' 'Benjamin Bennecker']]\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# check path location to make sure directory is correct\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "# make sure the file is in our directory\n",
        "if os.path.exists(\"astro_jeopardy_answers.csv\"):\n",
        "    print(\"astro_jeopardy_answers.csv exists in the current directory.\")\n",
        "else:\n",
        "    print(\"astro_jeopardy_answers.csv does not exist in the current directory.\")\n",
        "\n",
        "# read the file\n",
        "with open(\"astro_jeopardy_answers.csv\", \"r\") as file:\n",
        "    reader = csv.reader(file)\n",
        "    try:\n",
        "        answer = np.genfromtxt(\"astro_jeopardy_answers.csv\", delimiter=\",\", dtype=str)\n",
        "        print(\"Data array created successfully.\")\n",
        "        print(f\"The array's shape is {answer.shape}.\")\n",
        "        print(answer)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating data array: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "d4f6980b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "astro_jeopardy_facts.csv exists in the current directory.\n",
            "Data array created successfully.\n",
            "The array's shape is (23,).\n",
            "['HD189733b is a Hot Jupiter exoplanet that is known for its deadly conditions such as: raining molten glass sideways and containing a disgusting smell of rotten egg. This planet also contains an abundant amount of hydrogen sulfide and was discovered through radial velocity.'\n",
            " 'The Milky Way is the galaxy we live in! The Milky Way spans across more than 100,000 light-years. It takes our solar system around 250 million years to orbit our galaxy once!'\n",
            " 'Ursa Major is also known as the Great Bear and the third largest constellation. In Greek mythology, this bear is Callisto, a nymph of Artemis that Zeus is madly in love with.']\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# check path location to make sure directory is correct\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "# make sure the file is in our directory\n",
        "if os.path.exists(\"astro_jeopardy_facts.csv\"):\n",
        "    print(\"astro_jeopardy_facts.csv exists in the current directory.\")\n",
        "else:\n",
        "    print(\"astro_jeopardy_facts.csv does not exist in the current directory.\")\n",
        "\n",
        "# read the file\n",
        "# Note: np.genfromtxt expects the SAME number of columns on every row.\n",
        "# The header has no @ (1 column), but fact lines have @ at start (2 columns) -> \"got 2 columns instead of 1\"\n",
        "# Fix: read line-by-line and extract the fact text (everything after @)\n",
        "with open(\"astro_jeopardy_facts.csv\", \"r\") as file:\n",
        "    facts_list = []\n",
        "    next(file)  # skip header row\n",
        "    for line in file:\n",
        "        line = line.strip()\n",
        "        if line.startswith(\"@\"):\n",
        "            facts_list.append(line[1:].strip())  # remove @ and get fact text\n",
        "        # skip merge conflict lines and other non-fact lines\n",
        "    facts = np.array(facts_list)\n",
        "    print(\"Data array created successfully.\")\n",
        "    print(f\"The array's shape is {facts.shape}.\")\n",
        "    print(facts[:3])  # Show first 3 facts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "e9a38b6c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contacting LLM via University Server...\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "ERROR: Could not connect. Details:\n",
            "litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'model_dump'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      9\u001b[39m messages = [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: chat_assignment}, \n\u001b[32m     10\u001b[39m {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt}]\n\u001b[32m     12\u001b[39m response = prompt_llm(messages)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mshow_response_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mshow_response_metadata\u001b[39m\u001b[34m(response)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mConvert the response to a dictionary\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03mPrint information about token usage and costs\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Here are the top level keys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m response_dict = \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m()\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# print(f\"Top-level keys: {response_dict.keys()}\\n\")\u001b[39;00m\n\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Here are more details: \u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# 1. Get the exact model version used by the server\u001b[39;00m\n\u001b[32m     13\u001b[39m used_model = response.model\n",
            "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'model_dump'"
          ]
        }
      ],
      "source": [
        "chat_assignment = f\"\"\"You are Alex Trebek hosting a game of Astronomy-themed Jeopardy. Generate one Jeopardy-style\n",
        "     clue using the given facts, with the answer being {answer[1,2]} in the following category: {answer[0,2]}. \n",
        "     Do not mention the answer in the prompt, and only include the clue in your response.\"\"\"\n",
        "prompt = \"Ursa Major is also known as the Great Bear and the third largest constellation. In Greek mythology, this bear is Callisto, a nymph of Artemis that Zeus is madly in love with.\"\n",
        "# Next steps: make a for loop to run through each individual clue and answer in facts array. Then append each one to a \n",
        "# list and then make a numpy array. Read astro_jeopardy_facts.csv and also make it into a numpy array using the same \n",
        "# method as above. Replace the prompt variable with the corresponding row and column from the numpy array (the one with facts)\n",
        "\n",
        "messages = [{\"role\": \"system\", \"content\": chat_assignment}, \n",
        "{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "response = prompt_llm(messages)\n",
        "print(show_response_metadata(response))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
